{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1rKp85kKfO"
      },
      "source": [
        "# **GE - TD sur la détection de tumeurs dans des mammographies avec  Faster-RCNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTsW4vN3i4Z"
      },
      "source": [
        "# **1. Install les APIs pertinentes**\n",
        "\n",
        "Exécutez la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "emWVwRPi-8k0"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Install pycocotools\n",
        "git clone https://github.com/cocodataset/cocoapi.git\n",
        "cd cocoapi/PythonAPI\n",
        "python setup.py build_ext install\n",
        "cd ..\n",
        "cd ..\n",
        "\n",
        "# Install torchvision useful functions\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "\n",
        "cd ..\n",
        "\n",
        "pip install pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VavQFP2B3oz0"
      },
      "source": [
        "# **2. Import des packages Python pertinent pour le TD**\n",
        "\n",
        "Exécutez la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1"
      ],
      "metadata": {
        "id": "j5_4Sgt2CsAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgRF8ZmC_v9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSgyZU8z2oXq"
      },
      "source": [
        "# **3. La base de données MIAS**\n",
        "\n",
        "\n",
        "Les exercices de ce TP utiliseront tous la base données publique appelée\n",
        " **Mammographic Image Analysis Society (MIAS) dataset**.\n",
        "\n",
        "- Le dataset MIAS réunit **322** mammographies gauche et droite.\n",
        "- La taille d'une mammographie est de 1024x1024 pixels.\n",
        "- Chaque mammographie comporte plusieurs annotations (cf. le fichier Info.txt), par exemple:\n",
        "`mdb010 F CIRC B 525 425 33`\n",
        "                1st column: MIAS database reference number.\n",
        "\n",
        "                2nd column: Character of background tissue:\n",
        "                                F - Fatty\n",
        "                                G - Fatty-glandular\n",
        "                                D - Dense-glandular\n",
        "\n",
        "                3rd column: Class of abnormality present:\n",
        "                                CALC - Calcification\n",
        "                                CIRC - Well-defined/circumscribed masses\n",
        "                                SPIC - Spiculated masses\n",
        "                                MISC - Other, ill-defined masses\n",
        "                                ARCH - Architectural distortion\n",
        "                                ASYM - Asymmetry\n",
        "                                NORM - Normal\n",
        "\n",
        "                4th column: Severity of abnormality:\n",
        "                                B - Benign\n",
        "                                M - Malignant\n",
        "\n",
        "                5th,6th columns: x,y image-coordinates of centre of abnormality.\n",
        "\n",
        "                7th column: Approximate radius (in pixels) of a circle enclosing the abnormality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WKw1dkTVEwp"
      },
      "source": [
        "## 3.a. Télécharger le dataset MIAS\n",
        "\n",
        "Exécutez la cellule suivante pour télécharger le dataset MIAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC0vx6anVEwu"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Download the ddataset\n",
        "mkdir mias-db && cd mias-db\n",
        "wget http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz\n",
        "tar -zxvf all-mias.tar.gz\n",
        "rm all-mias.tar.gz && cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-OeJEh6giUe"
      },
      "source": [
        "## 3.b. Répartir le dataset entre train, validation et test sets.\n",
        "\n",
        "#### **Vous devez trouver les coordonnées des bounding box carrées englobant les tumeurs**.\n",
        "\n",
        "Dans le dataset MIAS une anormalité dans la mammographie est localisée par les coordonnées du pixel $(x, y)$ correspondant au centre de la lésion ainsi que le rayon $r$ en pixels du cercle englobant la lésion. En utilisant $(x, y)$ et $r$, vous pouvez en déduire les coordonnées de la bounding box carrée correspondante entourant la lésion.\n",
        "\n",
        "- Complétez la fonction `get_square_bounding_box(file_info)` qui retourne les coordonnées de la bounding box.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krEfU8BXVEw1"
      },
      "outputs": [],
      "source": [
        "# Path to database\n",
        "mias_db_path = './mias-db/'\n",
        "info_file = 'Info.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fY4rkZmVEw3"
      },
      "outputs": [],
      "source": [
        "def get_square_bounding_box(file_info):\n",
        "\n",
        "    if 'NORM' in file_info:\n",
        "        bbox = []\n",
        "    else:\n",
        "        x, y, r = int(file_info.split(' ')[4]), 1024-int(file_info.split(' ')[5]), int(file_info.split(' ')[6])\n",
        "        \"\"\"FILL HERE\"\"\"\n",
        "        bbox = ...\n",
        "    return bbox\n",
        "\n",
        "\n",
        "def read_dataset_info(mias_db_path, info_file, accepted_format=None, to_exclude=[]):\n",
        "    with open(os.path.join(mias_db_path, info_file), 'r') as fp:\n",
        "        if accepted_format is not None:\n",
        "            info = [f.strip() for f in fp.readlines() if f.startswith(accepted_format) and not f.startswith(to_exclude)]\n",
        "        else:\n",
        "            info = [f.strip() for f in fp.readlines() if f.startswith('mdb') and not f.startswith(to_exclude)]\n",
        "\n",
        "    dataset_info = {}\n",
        "    for file_info in info:\n",
        "        img_path = os.path.join(mias_db_path, file_info.split(' ')[0] + '.pgm')\n",
        "        class_name = \"NOTUMOUR\" if 'NORM' in file_info.split(' ')[2] else \"TUMOUR\"\n",
        "        bbox = get_square_bounding_box(file_info)\n",
        "\n",
        "        dataset_info[img_path] = {\n",
        "                                   \"class_name\": class_name,\n",
        "                                   \"bbox\": bbox}\n",
        "    return dataset_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAqjPA6NVEw5"
      },
      "outputs": [],
      "source": [
        "# Some cases contain abnormalities so we will exclude them.\n",
        "to_exclude = ('mdb216', 'mdb233', 'mdb245', 'mdb059')\n",
        "# Images to include in the validation set\n",
        "val_set = ('mdb001', 'mdb002', 'mdb005', 'mdb010', 'mdb012', 'mdb013')\n",
        "# Images to include in the test set\n",
        "test_set = ('mdb090', 'mdb091', 'mdb121', 'mdb134', 'mdb145', 'mdb218')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDjWt3hzVEw7"
      },
      "outputs": [],
      "source": [
        "train_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=None, to_exclude=to_exclude + val_set + test_set)\n",
        "validation_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=val_set, to_exclude=to_exclude)\n",
        "test_dataset = read_dataset_info(mias_db_path, info_file, accepted_format=test_set, to_exclude=to_exclude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_IFTrLPVEw9"
      },
      "outputs": [],
      "source": [
        "for img_path in test_dataset:\n",
        "\n",
        "    image = imageio.imread(img_path)\n",
        "    class_name = test_dataset[img_path]['class_name']\n",
        "    x1, y1, x2, y2  = test_dataset[img_path][\"bbox\"]\n",
        "\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'b-')\n",
        "    plt.axis('off')\n",
        "    plt.title(class_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfSr3u3xfoIg"
      },
      "source": [
        "## 3.c. Création de la classe `Dataset`.\n",
        "\n",
        "Pour créer un dataset vous allez devoir utiliser la classe Pytorch  `Dataset`. La structure de cette classe ressemble à ce qui suit:\n",
        "\n",
        "        class MyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "            def __init__(self, args):\n",
        "                ...\n",
        "\n",
        "            def __getitem__(self):\n",
        "                return pil_image, target\n",
        "\n",
        "            def __len(self)__:\n",
        "                return len(self.img_list)\n",
        "\n",
        "\n",
        "**i. Pour créer votre propre classe `Dataset`, vous allez override plusieurs méthodes**\n",
        "- le constructeur `__init__(self, args, transforms)`: Un détail important de cette méthode est qu'elle doit avoir prendre en entrée la variable `transforms`. Celle-ci sera utilisée par la méthode  `__getitem__(self, image_id)` pour faire de l'augmentation de données.\n",
        "- la méthode `__getitem__(self, image_id)`. Cette fonction doit retourner une image de type PIL, et un dictionnaire `target` qui contient les informations \"vérité terrain\" de l'image. Par exemple, le label de classification, les coordonnées de la bounding box... Les transformations liées à l'augmentation de données ont lieu dans cette fonction grâce à la varibale `transforms`.\n",
        "- la méthode `__len__(self)` qui retourne la taille du dataset.\n",
        "\n",
        "\n",
        "Dans votre classe, vous devez aussi coder:\n",
        "\n",
        "**ii. Dans le constructeur:**\n",
        "- `self.img_dict`: un dictionnaire qui associe l'indice `idx` d'une image aux informations la concernant (chemin vers l'image, coordonnées de la bounding box, label de classification). __Attention__: que se passe-t-il quand il n'y a pas de lésion dans la mammographie ? Il est nécessaire de créer de fausses bounding boxes.\n",
        "\n",
        "        self.img_dict[idx] = {'path': ...,\n",
        "                              'bbox': [...],\n",
        "                              'class': ...}\n",
        "                             \n",
        "- `self.class_names`: un dictionnaire qui associe le nom `string`des classes aux labels lus par le modèle.\n",
        "\n",
        "        self.class_names = {\"TUMOUR\": 1,\n",
        "                            \"NOTUMOUR\": 2}\n",
        "                            \n",
        "- `self.idx2class`: un dictionnaire qui fait l'inverse de `self.class_names`.\n",
        "\n",
        "**iii. Nouvelles méthodes:**\n",
        "- `add_random_bbox(self)`: cette fonction parcourt le dataset et créer des bounding box de façon aléatoire pour les mammographies ne présentant pas de lésion. Cela permerttra de donner des exemples de tissus sains au modèle pendant l'entraînement.\n",
        "- `load_image(self, img_idx)`: cette fonction retourne une image RGB de type PIL dont l'indice sera `img_idx`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qN2pB1BGUgo"
      },
      "outputs": [],
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split_dataset, transforms=None, name=\"train\"):\n",
        "        self.split_dataset = split_dataset\n",
        "        self.name = name\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.class_names = {\n",
        "            \"TUMOUR\": 1,\n",
        "            \"NOTUMOUR\": 2,\n",
        "        }\n",
        "\n",
        "        self.add_random_bbox()\n",
        "        self.idx2class = {self.class_names[name]: name for name in self.class_names}\n",
        "        self.img_dict = {img_idx:{\"path\":  img_path,\n",
        "                                  \"class\": self.class_names[self.split_dataset[img_path]['class_name']],\n",
        "                                  \"bbox\":  self.split_dataset[img_path]['bbox']}\n",
        "                          for img_idx, img_path in enumerate(self.split_dataset)}\n",
        "\n",
        "    def __getitem__(self, img_idx):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly.\n",
        "        \"\"\"\n",
        "        \"\"\"FILL HERE\n",
        "        Create the image, label and bbox variables using the implemented img_dict dictionary.\n",
        "        \"\"\"\n",
        "        image = ...\n",
        "        class_name = ...\n",
        "        bbox = ...\n",
        "\n",
        "        # Compute the area of the bounding box\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        area = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "        # Convert everything to tensor\n",
        "        img_idx = torch.tensor([img_idx])\n",
        "        bbox = torch.as_tensor([bbox], dtype=torch.float32)\n",
        "        class_name = torch.as_tensor([class_name], dtype=torch.int64)\n",
        "        area = torch.as_tensor([area], dtype=torch.float32)\n",
        "\n",
        "        # Compute the area of the bounding box\n",
        "#         area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])\n",
        "#         area = torch.as_tensor(area)\n",
        "\n",
        "        # Use the COCO template for targets to be able to evaluate the model with COCO API\n",
        "        target = {\"boxes\": bbox,\n",
        "                  \"labels\": class_name,\n",
        "                  \"image_id\": img_idx,\n",
        "                  \"area\": area,\n",
        "                  \"iscrowd\": torch.as_tensor([0], dtype=torch.int64)}\n",
        "\n",
        "        # Important line! don't forget to add this\n",
        "        if self.transforms:\n",
        "            image, target = self.transforms(image, target)\n",
        "        # return the image, the boxlist and the idx in your dataset\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.split_dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def add_random_bbox(self):\n",
        "        # Add a random bounding box for all images that are NORMAL.\n",
        "        for img_path in self.split_dataset:\n",
        "            if self.split_dataset[img_path]['class_name'] == \"NOTUMOUR\":\n",
        "\n",
        "                # where is there something in the mammogram ?\n",
        "                img = imageio.imread(img_path)\n",
        "\n",
        "                # Define random radius that should not be bigger than image\n",
        "                radius = np.random.randint(10, 70)\n",
        "\n",
        "                # Set borders at zero to avoid having a bounding box that is outside the mammogram\n",
        "                new_img = np.zeros(img.shape)\n",
        "                new_img[radius:-radius, radius:-radius] = img[radius:-radius, radius:-radius]\n",
        "                mask = new_img > 50\n",
        "                mask_id_x, mask_id_y = np.where(mask)\n",
        "\n",
        "                random_id_x, random_id_y = np.random.randint(len(mask_id_x)), np.random.randint(len(mask_id_y))\n",
        "                center_x, center_y = mask_id_x[random_id_x], mask_id_y[random_id_y]\n",
        "                bbox = [center_x-radius, center_y-radius, center_x+radius, center_y+radius]\n",
        "                self.split_dataset[img_path]['bbox'] = bbox\n",
        "\n",
        "\n",
        "\n",
        "    def load_image(self, img_idx):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly.\n",
        "        \"\"\"\n",
        "        img_path = self.img_dict[img_idx]['path']\n",
        "        image = imageio.imread(img_path)[..., np.newaxis]\n",
        "        image = np.concatenate((image, image, image), axis=2)\n",
        "        image = Image.fromarray(image).convert(\"RGB\")\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_qrOJVhY0P"
      },
      "source": [
        "## 3.d. Data augmentation\n",
        "\n",
        "La fonction `get_transform(train)` retourne les images aléatoirement pendant l'entraînement du modèle.\n",
        "\n",
        "Afin de renverser une image et d'appliquer la même transformation aux coordonnées de la bounding box, il faut coder une classe `RandomHorizontalFlip(object)`, dont la structure est montrée ci-dessous.\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "\n",
        "        def __init__(self, prob):\n",
        "            self.prob = prob\n",
        "\n",
        "        def __call__(self, image, target):\n",
        "            if random.random() < self.prob:\n",
        "                height, width = image.shape[-2:]\n",
        "                # Flip image\n",
        "                image = image.flip(-1)\n",
        "                # Flip bounding box coordinates\n",
        "                bbox = target[\"boxes\"]\n",
        "                bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "                target[\"boxes\"] = bbox\n",
        "            return image, target\n",
        "\n",
        "Vous pouvez coder vos propre fonction d'augmentaiton de données si vous le souhaitez.\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtjdUdBnrRxC"
      },
      "outputs": [],
      "source": [
        "import transforms as T\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hi16zOkhgxH"
      },
      "source": [
        "## 3.e. Création des train, validation et test sets\n",
        "\n",
        "- Créez les train, validation and test sets en utilisant la classe `CancerDataset`.\n",
        "- A quel point le training set est-il déséquilibré ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCrJ9WwArh8t"
      },
      "outputs": [],
      "source": [
        "\"\"\" FILL HERE\n",
        "create your train val and test datasets\n",
        "\"\"\"\n",
        "train = ...\n",
        "val = ...\n",
        "test = ...\n",
        "\n",
        "print(\"Number of images in training set: {}\".format(len(train)))\n",
        "print(\"Number of images in validation set: {}\".format(len(val)))\n",
        "print(\"Number of images in test set: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSifbn2VExg"
      },
      "outputs": [],
      "source": [
        "# How imbalanced is the training set ?\n",
        "class_counts = {\"TUMOUR\": 0, \"NOTUMOUR\": 0}\n",
        "for img_idx in train.img_dict:\n",
        "    class_name = train.idx2class[train.img_dict[img_idx]['class']]\n",
        "    if \"NO\" in class_name: class_counts['NOTUMOUR'] += 1\n",
        "    else: class_counts['TUMOUR'] += 1\n",
        "\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26CJAXSD4tt1"
      },
      "source": [
        "## 3.f. Visualisez quelques images et leurs bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcYC2DS8VExi"
      },
      "outputs": [],
      "source": [
        "for img_idx in range(20, 25):\n",
        "\n",
        "    img_path = train.img_dict[img_idx]['path']\n",
        "    image = imageio.imread(img_path)\n",
        "    class_name = train.idx2class[train.img_dict[img_idx]['class']]\n",
        "    x1, y1, x2, y2  = train.img_dict[img_idx][\"bbox\"]\n",
        "\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    if \"NOTUMOUR\" in class_name:\n",
        "        plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'g-')\n",
        "    else:\n",
        "        plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'r-')\n",
        "    plt.axis('off')\n",
        "    plt.title(class_name)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKZ19CQq4zQp"
      },
      "source": [
        "## 3.g. Create the data loaders\n",
        "\n",
        "\n",
        "In this section, we instantiate **data loaders** that will be used to generate batches of images on the fly during training.\n",
        "\n",
        "For each of the created datasets, you need to call `torch.utils.data.DataLoader` and define :\n",
        "- the `batch_size`,\n",
        "- whether to randomly shuffle the dataset so the dataloaders will return random samples by setting the `shuffle` parameter to `True` or `False`. Typically, you want to shuffle your training dataset. It does not matter for the validation and test sets.\n",
        "- the number of processes that should be used to load each batch using `num_workers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NW-LYe4gGbO"
      },
      "outputs": [],
      "source": [
        "# Data loaders\n",
        "# torch.manual_seed(1)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train, batch_size=8, shuffle=True, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    val, batch_size=1, shuffle=False, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test, batch_size=1, shuffle=False, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAYljfrmzMvx"
      },
      "source": [
        "# **4. Faster Region-based Convolutional Network model - [code](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py), [article](https://arxiv.org/abs/1506.01497)**\n",
        "\n",
        "L'architecture du modèle **FasterRCNN** comprend le RPN comme algorithme proposant des régions d'intérêt et le modèle Fast RCNN comme détecteur.\n",
        "\n",
        "Dans cet exercice nous entraînerons un **Faster R-CNN model avec un ResNet-50-FPN backbone** à la place de VGG.\n",
        "\n",
        "![](https://bit.ly/3BoJCjj)\n",
        "\n",
        ">  L'entrée du modèle doit être une liste de tensors (un par image), chacun de dimensions `[C, H, W]`  (nombre de canaux de couleur d'abord) et dont les valeurs devraient être comprise environ entre à et 1.  Les images peuvent ne pas avoir les mêmes dimensions.\n",
        "\n",
        "> Pendant l'**entraînement**, le modèle prend la liste de tensors et les dictionnaires `targets` correspondants i.e. une liste de dictionnaires. Chaque dictionnaire `target` contients:\n",
        "> - `boxes` (`FloatTensor` de dimmensions `[N, 4]`): les coordonnées des bounding boxes de référents sous format [x1, y1, x2, y2], avec `0 <= x1 < x2 <= W` et `0 <= y1 < y2 <= H.`\n",
        "> - `labels` (`Int64Tensor` de dimmension `[N]`): le label de classification pour chaque bounding box de référence.\n",
        ">- `image_id` (`Int64Tensor` de dimmension `[1]`): l'indice identifiant l'image. Il doit être unique pour chaque image contenue dans le dataset considéré\n",
        ">- `area` (`Tensor` de dimmension `[N]`): l'aire des bounding boxes. Elle est utilisée pendant l'évaluation, pour séparer les résultats des métriques en fonction de la taille des bounding boxes.\n",
        "\n",
        "> Le modèle retourne un `Dict[Tensor]` pendant l'entraînement, qui contient le label estimé de classification et les pertes de régression pour les coordonnées des bounding boxes pour le RPN et pour le RCNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbXJcYQVExn"
      },
      "source": [
        "## Exhaustive list of Faster RCNN's input arguments\n",
        "\n",
        "**About the architecture:**\n",
        "> - `backbone (nn.Module)`: the network used to compute the features for the model.\n",
        "    It should contain a out_channels attribute, which indicates the number of output\n",
        "    channels that each feature map has (and it should be the same for all feature maps).\n",
        "    The backbone should return a single `Tensor` or an `OrderedDict[Tensor]`.\n",
        "    \n",
        "**About the input data:**\n",
        "> Classes:\n",
        "> - `num_classes (int)`: number of output classes of the model (including the background).\n",
        "    If box_predictor is specified, `num_classes` should be None.\n",
        "\n",
        "> Image size rescaling:\n",
        "> - `min_size (int)`: minimum size of the image to be rescaled before feeding it to the backbone\n",
        "> - `max_size (int)`: maximum size of the image to be rescaled before feeding it to the backbone\n",
        "\n",
        "> Image normalization:\n",
        "> - `image_mean (Tuple[float, float, float])`: mean values used for input normalization.\n",
        "    They are generally the mean values of the dataset on which the backbone has been trained\n",
        "    on\n",
        "> - `image_std (Tuple[float, float, float])`: std values used for input normalization.\n",
        "    They are generally the std values of the dataset on which the backbone has been trained on\n",
        "    \n",
        "**About the Region Proposal Network (RPN):**\n",
        "\n",
        "> Architecture:\n",
        ">- `rpn_anchor_generator (AnchorGenerator)`: module that generates the anchors for a set of feature\n",
        "    maps.\n",
        ">- `rpn_head (nn.Module)`: module that computes the objectness and regression deltas from the RPN\n",
        "\n",
        "> NMS parameters:\n",
        "> - `rpn_pre_nms_top_n_train (int)`: number of proposals to keep before applying NMS during training\n",
        "> - `rpn_pre_nms_top_n_test (int)`: number of proposals to keep before applying NMS during testing\n",
        "> - `rpn_post_nms_top_n_train (int)`: number of proposals to keep after applying NMS during training\n",
        "> - `rpn_post_nms_top_n_test (int)`: number of proposals to keep after applying NMS during testing\n",
        "> - `rpn_nms_thresh (float)`: NMS threshold used for postprocessing the RPN proposals\n",
        "\n",
        "> IoU thresholds:\n",
        ">- `rpn_fg_iou_thresh (float)`: minimum IoU between the anchor and the GT box so that they can be\n",
        "    considered as positive during training of the RPN.\n",
        ">- `rpn_bg_iou_thresh (float)`: maximum IoU between the anchor and the GT box so that they can be\n",
        "    considered as negative during training of the RPN.\n",
        "\n",
        "> RPN parameters for training:\n",
        ">- `rpn_batch_size_per_image (int)`: number of anchors that are sampled during training of the RPN\n",
        "    for computing the loss\n",
        ">- `rpn_positive_fraction (float)`: proportion of positive anchors in a mini-batch during training\n",
        "    of the RPN\n",
        "\n",
        "> RPN parameter for inference:\n",
        "> - `rpn_score_thresh (float)`: during inference, only return proposals with a classification score\n",
        "    greater than `rpn_score_thresh`\n",
        "    \n",
        "**About bounding box processing and proposals:**\n",
        "\n",
        "> Architecture:\n",
        ">- `box_roi_pool (MultiScaleRoIAlign)`: the module which crops and resizes the feature maps in\n",
        "    the locations indicated by the bounding boxes\n",
        ">- `box_head (nn.Module)`: module that takes the cropped feature maps as input\n",
        ">- `box_predictor (nn.Module)`: module that takes the output of box_head and returns the\n",
        "    classification logits and box regression deltas.\n",
        "\n",
        "> Inference:\n",
        ">- `box_score_thresh (float)`: during inference, only return proposals with a classification score\n",
        "    greater than `box_score_thresh`\n",
        "> - `box_nms_thresh (float)`: NMS threshold for the prediction head. Used during inference\n",
        "> - `box_detections_per_img (int)`: maximum number of detections per image, for all classes.\n",
        "\n",
        "> Training:\n",
        ">- `box_fg_iou_thresh (float)`: minimum IoU between the proposals and the GT box so that they can be\n",
        "    considered as positive during training of the classification head\n",
        ">- `box_bg_iou_thresh (float)`: maximum IoU between the proposals and the GT box so that they can be\n",
        "    considered as negative during training of the classification head\n",
        ">- `box_batch_size_per_image (int)`: number of proposals that are sampled during training of the\n",
        "    classification head\n",
        ">- `box_positive_fraction (float)`: proportion of positive proposals in a mini-batch during training\n",
        "    of the classification head\n",
        ">- `bbox_reg_weights (Tuple[float, float, float, float])`: weights for the encoding/decoding of the\n",
        "    bounding boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pn1RrTbVExo"
      },
      "source": [
        "## 4.a. Normalisation des données\n",
        "\n",
        "\n",
        "Calculez la valeur moyenne `image_mean` et l'écart type `image_std` du training set.\n",
        "\n",
        "Les sorties des fonctions devra être un **tuple**, un élément (moyenne ou écart type) par canal couleur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLE1Vsj8r5Vl"
      },
      "outputs": [],
      "source": [
        "def compute_means(dataset):\n",
        "    \"\"\" FILL HERE \"\"\"\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl2iHaA3s3bn"
      },
      "outputs": [],
      "source": [
        "def compute_stds(dataset):\n",
        "    \"\"\" FILL HERE \"\"\"\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F6TzeNHs8vF"
      },
      "outputs": [],
      "source": [
        "image_mean = compute_means(train)\n",
        "image_std = compute_stds(train)\n",
        "\n",
        "\n",
        "print(\"Means: {}\".format(image_mean))\n",
        "print(\"Stds: {}\".format(image_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4GGLeRrVExs"
      },
      "source": [
        "## 4.b. Création du modèle.\n",
        "\n",
        "Pour créer le modèle, vous devrez:\n",
        "- appeler `torchvision.models.detection.fasterrcnn_resnet50_fpn` et lui donnér la moyenne et l'écart type du training set.\n",
        "- indiquer le nombre de classes de notre problème:  `NOTUMOUR`, `TUMOUR` et une classe pour le fond de l'image donc 3 classes.\n",
        "- remplacer la tête du prédicteur de bounding box par FastRCNN `FastRCNNPredictor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcomEfZr2Qe"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                             image_mean=image_mean,\n",
        "                                                             image_std=image_std)\n",
        "\n",
        "# replace the classifier with a new one, that has num_classes which is user-defined\n",
        "num_classes = 3  # intumomur + no tumour + background\n",
        "\n",
        "# get number of input channels for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkSNvqkjSHYu"
      },
      "source": [
        "## 4.c. Calculez le nombre de paramêtres du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPsCQAxk-7yO"
      },
      "outputs": [],
      "source": [
        "\"\"\" FILL HERE \"\"\"\n",
        "model_parameters = ...\n",
        "params = ...\n",
        "print(\"Number of trainable parameters: {:.4e}\".format(params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIJMa4O5zJNS"
      },
      "source": [
        "# **5. Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCMGV-XDXpJd"
      },
      "source": [
        "## 5.a. Optimiseur et Hyperparamètres\n",
        "\n",
        "> Définissez l'optimiseur et les hyperparamètres associés à utiliser pendant l'entraînement:\n",
        ">- learning rate initial,\n",
        ">- momentum,\n",
        ">- weight decay,\n",
        ">- ...\n",
        "\n",
        "> utilisez un learning rate scheduler pour réduire le learning rate progressivement pendant le training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uapHMOU3dmO3"
      },
      "outputs": [],
      "source": [
        "# gpu_number = 3\n",
        "\n",
        "# torch.cuda.set_device(gpu_number)\n",
        "# move model to the right device\n",
        "model.cuda()\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "\"\"\" FILL HERE\"\"\"\n",
        "optimizer = ...\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate\n",
        "# Change the scheduler type if you wish\n",
        "lr_scheduler = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frGCNluVT-ct"
      },
      "source": [
        "## 5.b. Fonction de training et validation pour une époque.\n",
        "\n",
        ">  Implementez votre propre fonction d'entraînement pour une seule époque. La fonction devra:\n",
        ">- Parcourir le dataloader et sélectionner les batches,\n",
        ">- Donner la prédiction du modèle,\n",
        ">- Calculez la fonction de perte (la loss),\n",
        ">- Re-initialiser l'optimiseur: pour chque mini-batch, il faut explicitement annuler tous les gradients avant de commencer la rétropropagation parce que Pytorch accumule les gradients calculés précédemment. Si on ne fait pas ça on risque une erreur `Out-Of-Memory`.\n",
        ">- Faire la rétropropagation  (calcule les gradients for every pour chque paramètre dans le modèle).\n",
        ">- Faire les mise-à-jour des paramètres (update les poids et biais du modèle en utilisant les gradients calculés).\n",
        "\n",
        "> Implementez une fonction similaire mais destinées au calcul de la loss sur le validation set. Pas besoin de calculer des gradients, de rétropropager ou de faire des mises-à-jour dans cette fonction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Jjniq7VExz"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiJLUg7uVEx0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcK8Gisnljfe"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, epoch, writer):\n",
        "\n",
        "    # Set the model in training mode: the gradients will be saved.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = {}\n",
        "    for i, values in enumerate(data_loader):\n",
        "        images, targets = values\n",
        "\n",
        "        # Create list of input images\n",
        "        images = ...\n",
        "        # Create list of ground-truth dicionnaries\n",
        "        targets = ...\n",
        "\n",
        "        # Feed the training samples to the model\n",
        "        # The model returns loss_dict which contains the values of every loss functions\n",
        "        loss_dict = ...\n",
        "        # Compute the global loss by summing all loss values\n",
        "        global_loss = ...\n",
        "        loss_value = ...\n",
        "\n",
        "        # Increment the epoch's loss\n",
        "        for k, v in loss_dict.items():\n",
        "            epoch_loss[k] = epoch_loss.get(k, []) + [v.item()]\n",
        "        epoch_loss['global_loss'] = epoch_loss.get('global_loss', []) + [loss_value]\n",
        "\n",
        "        # If your loss is a Nan or infinite, you need to stop the training because it's failing.\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        ...\n",
        "        # Backpropagation: compute gradients\n",
        "        ...\n",
        "        # Update the model's parameters\n",
        "        ...\n",
        "\n",
        "    # Compute the losses over the whole epoch\n",
        "    for k, v in epoch_loss.items():\n",
        "        epoch_loss[k] = np.mean(v)\n",
        "        writer.add_scalar('Training Loss/{}'.format(k), np.mean(v), epoch)\n",
        "    writer.flush()\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moortphv6zix"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, data_loader, epoch, writer):\n",
        "\n",
        "    validation_loss = {}\n",
        "    for i, values in enumerate(data_loader):\n",
        "        images, targets = values\n",
        "        images = list(image.cuda() for image in images)\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        global_loss = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = global_loss.item()\n",
        "\n",
        "        # Increment the epoch's loss\n",
        "        for k, v in loss_dict.items():\n",
        "            validation_loss[k] = validation_loss.get(k, []) + [v.item()]\n",
        "        validation_loss['global_loss'] = validation_loss.get('global_loss', []) + [loss_value]\n",
        "\n",
        "    # Compute the losses over the whole epoch\n",
        "    for k, v in validation_loss.items():\n",
        "        validation_loss[k] = np.mean(v)\n",
        "        writer.add_scalar('Validation Loss/{}'.format(k), np.mean(v), epoch)\n",
        "    writer.flush()\n",
        "    return validation_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMc5tV1Vc2W"
      },
      "source": [
        "## 5.c. Entraînez le modèle\n",
        "\n",
        "Définissiez le nombre d'époques `num_epochs`pendant lesquelles on entraîne le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxYmwb1Ydout"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "\n",
        "    # Train for one epoch, printing every 10 iterations\n",
        "    start = time.time()\n",
        "    epoch_loss = train_one_epoch(model, optimizer, train_data_loader, epoch, writer)\n",
        "    result = \"Epoch {} [{:.1f} s] - lr: {:.3e}:\".format(epoch, time.time()-start, lr_scheduler.get_last_lr()[0])\n",
        "    for k, v in epoch_loss.items(): result += \"\\t{}: {:.6f}\".format(k, v)\n",
        "    print(result)\n",
        "\n",
        "    # Compute losses over the validation set\n",
        "    validation_loss = validate_one_epoch(model, val_data_loader, epoch, writer)\n",
        "    result = \"Validation:\"\n",
        "    for k, v in validation_loss.items(): result += \"\\t{}: {:.6f}\".format(k, v)\n",
        "    print(result)\n",
        "\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Save the model if you wish. You can add a criteria before saving, for example\n",
        "    # if the validation decreases.\n",
        "    # save_path = \"/content/drive/My Drive/mva_td/saved_models/my_model\"\n",
        "    # torch.save(model, save_path)\n",
        "\n",
        "#     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKHiz112AEC"
      },
      "source": [
        "## 5.d. Visualiser les courbes d'apprentissage avec Tensorboard\n",
        "\n",
        "Est-ce que le modèle overfit ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrBn8OOMdCVo"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir './runs/'\n",
        "\n",
        "from tensorboard import notebook\n",
        "notebook.list() # View open TensorBoard instances\n",
        "notebook.display(port=6006, height=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO7numj-VEx6"
      },
      "outputs": [],
      "source": [
        "!tensorboard --logdir=runs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-HSSRTyzDNL"
      },
      "source": [
        "# **6. Evaluation over the test set**\n",
        "\n",
        "> Pendant la phase d'**inference**, la modèle ne prend en entrée que la liste de tensors et retourne les prédictions sour la forme d'une liste de dictionnaires `List[Dict[Tensor]]`, un dictionnaire par iamge d'entrée. Les champs d'un dictionnaire sont les suivants, N étant e nombre de détections effectuées:\n",
        "> - `boxes` (`FloatTensor` de dimensions `[N, 4]`): les boxes estimées par le modèle sous format `[x1, y1, x2, y2]`, avec `0 <= x1 < x2 <= W` et `0 <= y1 < y2 <= H`.\n",
        "> - `labels` (`Int64Tensor` de dimension `[N]`): les labels de classification prédit pour chaque bounding box détectée.\n",
        "> - `scores` (`Tensor` de dimension `[N]`): le score de confiance du modèle pour chaque détection.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeJMJpkQWrrV"
      },
      "source": [
        "## 6.a. Visualisation des prédictions\n",
        "\n",
        "\n",
        "En mode \"évaluation\", le modèle n'a pas accès aux `targets` de référence. Pour passer dans ce mode, il faut utiliser la syntaxe `model.eval()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-CsgHXdVEx8"
      },
      "outputs": [],
      "source": [
        "def visualize(image, target, prediction):\n",
        "\n",
        "\n",
        "    gt_bbox = target['boxes'][0].cpu().numpy()\n",
        "    gt_label = target['labels'].cpu().numpy()\n",
        "\n",
        "    pred_bboxs = prediction['boxes'].cpu().detach().numpy()\n",
        "    pred_labels = prediction['labels'].cpu().detach().numpy()\n",
        "    pred_scores = prediction['scores'].cpu().detach().numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Plot mammogram\n",
        "    image = image.mul(255).permute(1, 2, 0).cpu().byte()\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot ground-truth bounding box\n",
        "    x1, y1, x2, y2 = gt_bbox\n",
        "    line1, = plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], 'b-', label='Ground-truth')\n",
        "\n",
        "    # Plot predicted bounding boxes\n",
        "    for i in range(len(pred_labels)):\n",
        "        x1, y1, x2, y2 = pred_bboxs[i]\n",
        "        score = pred_scores[i]\n",
        "\n",
        "        if pred_labels[i] == 1:\n",
        "            c = 'r'\n",
        "            label='Prediction: tumour'\n",
        "            plt.annotate('{:.1f}'.format(100*score), (x1, y1), c=c, fontsize='medium')\n",
        "        else:\n",
        "            c = 'g'\n",
        "            label='Prediction: no tumour'\n",
        "            plt.annotate('{:.1f}'.format(100*score), (x2, y2), c=c, fontsize='medium')\n",
        "        line, = plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], c, label=label)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcISZFIdVEx9"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "for i, values in enumerate(test_data_loader):\n",
        "        images, targets = values\n",
        "\n",
        "        # Create list of input images\n",
        "        images = list(image.cuda() for image in images)\n",
        "        # Create list of ground-truth dicionnaries\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Feed the training samples to the model\n",
        "        # The model returns  the predictions\n",
        "        predictions = model(images)\n",
        "\n",
        "        visualize(images[0], targets[0], predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSZ7MyBYy4VJ"
      },
      "source": [
        "## 6.b. Evaluation\n",
        "\n",
        "Exécutez le code suivant pour calculer la métrique \"mean Average Precision\" sur le test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jaMaBEhzRkQ"
      },
      "outputs": [],
      "source": [
        "from engine import evaluate\n",
        "device = torch.device('cuda:{}'.format(gpu_number))\n",
        "evaluate(model, test_data_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbkY8aN3p9RX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}